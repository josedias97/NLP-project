# -*- coding: utf-8 -*-
"""NLP Project -  Best Model.ipynb

Automatically generated by Colaboratory.


### Best Model
"""

import pandas as pd
from tqdm import tqdm_notebook as tqdm
import nltk
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.stem import SnowballStemmer
from bs4 import BeautifulSoup
import string
import re
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten,BatchNormalization, Conv1D, Embedding,MaxPooling1D, Add, Bidirectional,GlobalMaxPooling1D,SpatialDropout1D,GlobalAveragePooling1D,Activation, Input, Masking, TimeDistributed, LSTM, GRU,BatchNormalization
from keras import models, layers, metrics, regularizers
import string as st
from nltk import PorterStemmer
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
import pandas as pd
import string
import keras
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras.models import load_model
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras import backend as K
from keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix as cm, classification_report as cr
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from keras.initializers import Constant
from keras.layers import Reshape, merge, Concatenate, Lambda, Average,concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Bidirectional
from datetime import datetime
from google.colab import files
from imblearn.pipeline import Pipeline, make_pipeline

url = 'https://raw.githubusercontent.com/josedias97/NLP-project/main/training_set.txt'
url_1 = 'https://raw.githubusercontent.com/josedias97/NLP-project/main/dev_set.txt'
url_2 = 'https://raw.githubusercontent.com/josedias97/NLP-project/main/test_set.txt'

df_train = pd.read_csv(url, sep="\t")
df_val = pd.read_csv(url_1, sep="\t")
df_test = pd.read_csv(url_2, sep="\t")

df_test_backup = df_test.copy()
df_val_backup = df_val.copy()

print(df_train.shape)
print(df_val.shape)
print(df_test.shape)

all_train_data = pd.concat([df_train, df_val], axis=0)

all_train_data.shape

df_train.head()

df_val.head()

"""### Metrics computing"""

def f1_metric(y_true, y_pred):

    def recall(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())
        return recall

    def precision(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())
        return precision

    precision = precision(y_true, y_pred)
    recall = recall(y_true, y_pred)
    #'Recall:',recall, 'Precision:', precision, 'F1:',
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

def f1_metric_precision_recall(y_true, y_pred):

    def recall(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())
        return recall

    def precision(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())
        return precision

    precision = precision(y_true, y_pred)
    recall = recall(y_true, y_pred)

    return 'Recall:',recall, 'Precision:', precision, 'F1:',2*((precision*recall)/(precision+recall+K.epsilon()))

def confusion_matrix(data_set, predictions):
    print('Confusion matrix')
    return cm(data_set, predictions)

def classification_report(data_set, predictions):
    print('Classification Report')
    target_names = ["Anger", "Anticipation", "Disgust", "Fear", "Joy", "Sadness", "Surprise", "Trust"]
    print(cr(data_set, predictions, target_names=target_names))

"""### Callbacks"""

callbacks_list = [
    keras.callbacks.EarlyStopping( # Once triggered, stops the training (if the val_accuracy decreases 3 epochs in a row).
        monitor='val_accuracy',
        patience=3),
    keras.callbacks.ModelCheckpoint( # Saves the best model observed during training (according to val_loss).
        filepath = 'cnn_model_file',
        monitor='val_loss',
        save_best_only=True)
]

earlystopping = EarlyStopping(monitor ="val_accuracy",  
                                        mode ="max", 
                                        patience =3,
                                        verbose=1,  
                                        restore_best_weights = True)

my_callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=3),
    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),
    tf.keras.callbacks.TensorBoard(log_dir='./logs'),
]

"""### Label Counter"""

def label_counter(dataframe, field):
    """
    Function that receives a dataframe and the field whose labels you want to count, and
    returns the amount of examples with those labels in the Pandas dataframe.
    """    
    return dataframe[field].value_counts()

label_counter(df_train, "emotion")

"""### Word Counter"""

def word_counter(text_list):
    """
    Function that receives a list of strings and returns the (absolute) frequency of each word in that list of strings.
    """
    words_in_df = ' '.join(text_list).split()
    
    # Count all words 
    freq = pd.Series(words_in_df).value_counts()
    return freq

word_counter(list(df_train['sentence']))[:25]

"""### NLP Pipeline

### Initial Preprocessing
"""

def remove_punct(text):
    return ("".join([ch for ch in text if ch not in st.punctuation]))

df_train['removed_punc'] = df_train['sentence'].apply(lambda x: remove_punct(x))
df_val['removed_punc'] = df_val['sentence'].apply(lambda x: remove_punct(x))
all_train_data['removed_punc'] = all_train_data['sentence'].apply(lambda x: remove_punct(x))
df_test['removed_punc'] = df_test['sentence'].apply(lambda x: remove_punct(x))

df_train.head()

def tokenize(text):
    text = re.split('\s+' ,text)
    return [x.lower() for x in text]

df_train['tokens'] = df_train['removed_punc'].apply(lambda msg : tokenize(msg))
df_val['tokens'] = df_val['removed_punc'].apply(lambda msg : tokenize(msg))
all_train_data['tokens'] = all_train_data['removed_punc'].apply(lambda msg : tokenize(msg))
df_test['tokens'] = df_test['removed_punc'].apply(lambda msg : tokenize(msg))

df_train.head()

def remove_small_words(text):
    return [x for x in text if len(x) > 0 ]

df_train['larger_tokens'] = df_train['tokens'].apply(lambda x : remove_small_words(x))
df_val['larger_tokens'] = df_val['tokens'].apply(lambda x : remove_small_words(x))
all_train_data['larger_tokens'] = all_train_data['tokens'].apply(lambda x : remove_small_words(x))
df_test['larger_tokens'] = df_test['tokens'].apply(lambda x : remove_small_words(x))

df_train.head()

def remove_stopwords(text):
    return [word for word in text if word not in nltk.corpus.stopwords.words('english')]

nltk.download('stopwords')

df_train['clean_tokens'] = df_train['larger_tokens'].apply(lambda x : remove_stopwords(x))
df_val['clean_tokens'] = df_val['larger_tokens'].apply(lambda x : remove_stopwords(x))
all_train_data['clean_tokens'] = all_train_data['larger_tokens'].apply(lambda x : remove_stopwords(x))
df_test['clean_tokens'] = df_test['larger_tokens'].apply(lambda x : remove_stopwords(x))

df_train.head()

def stemming(text):
    ps = PorterStemmer()
    return [ps.stem(word) for word in text]

df_train['stem_words'] = df_train['clean_tokens'].apply(lambda wrd: stemming(wrd))
df_val['stem_words'] = df_val['clean_tokens'].apply(lambda wrd: stemming(wrd))
all_train_data['stem_words'] = all_train_data['clean_tokens'].apply(lambda wrd: stemming(wrd))
df_test['stem_words'] = df_test['clean_tokens'].apply(lambda wrd: stemming(wrd))

df_train.head()

def lemmatize(text):
    word_net = WordNetLemmatizer()
    return [word_net.lemmatize(word) for word in text]

nltk.download('wordnet')

df_train['lemma_words'] = df_train['clean_tokens'].apply(lambda x : lemmatize(x))
df_val['lemma_words'] = df_val['clean_tokens'].apply(lambda x : lemmatize(x))
all_train_data['lemma_words'] = all_train_data['clean_tokens'].apply(lambda x : lemmatize(x))
df_test['lemma_words'] = df_test['clean_tokens'].apply(lambda x : lemmatize(x))

df_train.head()

def return_sentences(tokens):
    return " ".join([word for word in tokens])

df_train['clean_text'] = df_train['lemma_words'].apply(lambda x : return_sentences(x))
df_val['clean_text'] = df_val['lemma_words'].apply(lambda x : return_sentences(x))
all_train_data['clean_text'] = all_train_data['lemma_words'].apply(lambda x : return_sentences(x))
df_test['clean_text'] = df_test['lemma_words'].apply(lambda x : return_sentences(x))

df_train.head()

df_val.head()

df_train = df_train[['emotion', 'clean_text']]

df_val = df_val[['emotion', 'clean_text']]

all_train_data = all_train_data[['emotion', 'clean_text']]

del df_test['sentence']

df_test.rename(columns = {'clean_text':'sentence'}, inplace=True)

df_test = df_test['sentence']

df_train.rename(columns = {'clean_text':'sentence'}, inplace=True)
df_val.rename(columns = {'clean_text':'sentence'}, inplace=True)
all_train_data.rename(columns = {'clean_text':'sentence'}, inplace=True)

X_train = df_train['sentence']
y_train = df_train['emotion']

X_val = df_val['sentence']
y_val = df_val['emotion']

X_all_train_data = all_train_data['sentence']
y_all_train_data = all_train_data['emotion']

"""### Weak Baseline - KNN"""

model25 =Pipeline([
  ('vect', CountVectorizer()),
   ('knn', KNeighborsClassifier())
])

model25.fit(X_train, y_train)
y_pred = model25.predict(X_val)
y_train_pred = model25.predict(X_train)
co_mat = confusion_matrix(y_val,y_pred)

print(co_mat)
print(classification_report(y_val,y_pred))

"""### Preprocessing for the Deep Learning Models"""

# tokenizing
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_val = tokenizer.texts_to_sequences(X_val)

tokenizer_2 = Tokenizer()
tokenizer_2.fit_on_texts(X_all_train_data)
X_all_train_data = tokenizer_2.texts_to_sequences(X_all_train_data)
df_test = tokenizer_2.texts_to_sequences(df_test)

maxLength = 60

word_index = tokenizer.word_index
len(word_index)

word_index_2 = tokenizer_2.word_index
len(word_index_2)

X_padded = pad_sequences(X_train,padding='post', maxlen=maxLength)

X_val_padded = pad_sequences(X_val,padding='post', maxlen=maxLength)

X_all_train_data_padded = pad_sequences(X_all_train_data,padding='post', maxlen=maxLength)

df_test_padded = pad_sequences(df_test,padding='post', maxlen=maxLength)

sanityCheckIndex={v: k for k, v in tokenizer.word_index.items()}
print(X_train[500])
print(' '.join([sanityCheckIndex[wordIndex] for wordIndex in X_train[500]]))
print(X_padded[500][0])
print(X_padded[500][-1])
print(' '.join([sanityCheckIndex[wordIndex] for wordIndex in X_padded[500] if wordIndex!=0 ]))

tokenizer.word_index.items()

# Label encoding
enc = LabelEncoder()
y_train=enc.fit_transform(y_train)
y_train=to_categorical(y_train)

y_val=enc.fit_transform(y_val)
y_val=to_categorical(y_val)

y_all_train_data=enc.fit_transform(y_all_train_data)
y_all_train_data=to_categorical(y_all_train_data)

!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip glove*.zip.2

!ls
!pwd

# Using Glove for word embeddings
word_index = tokenizer.word_index
EMBEDDING_DIM = 100 # 100 dimensional embedding matrix is imported (100 features)

embeddings_index = {}

#f = open('/content/drive/My Drive/globe.6B/glove.6B.100d.txt')
f = open('glove.6B.100d.txt', encoding='utf-8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s unique tokens.' % len(word_index))  # dictionary length
print('Total %s word vectors.' % len(embeddings_index))  # words present in the imported word embeddings

word_index = tokenizer.word_index
embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM)) # initializing the embedding matrix
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)  # getting the word vector from glove for the words present in our vocab and constructing the embedding matrix
    if embedding_vector is not None:    # to account for words for which the glove doesnt have a representation
        embedding_matrix[i] = embedding_vector

embedding_matrix.shape

# predefining the embedding layer
embedding_layer = Embedding(len(word_index)+1,
                            EMBEDDING_DIM,
                            embeddings_initializer=Constant(embedding_matrix),
                            input_length=maxLength,
                            trainable=False)

# Using Glove for word embeddings
word_index_2 = tokenizer_2.word_index
EMBEDDING_DIM = 100 # 100 dimensional embedding matrix is imported (100 features)

embeddings_index = {}

#f = open('/content/drive/My Drive/globe.6B/glove.6B.100d.txt')
f = open('glove.6B.100d.txt', encoding='utf-8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s unique tokens.' % len(word_index_2))  # dictionary length
print('Total %s word vectors.' % len(embeddings_index))  # words present in the imported word embeddings

word_index_2 = tokenizer_2.word_index
embedding_matrix = np.zeros((len(word_index_2) + 1, EMBEDDING_DIM)) # initializing the embedding matrix
for word, i in word_index_2.items():
    embedding_vector = embeddings_index.get(word)  # getting the word vector from glove for the words present in our vocab and constructing the embedding matrix
    if embedding_vector is not None:    # to account for words for which the glove doesnt have a representation
        embedding_matrix[i] = embedding_vector

embedding_matrix.shape

# predefining the embedding layer
embedding_layer_2 = Embedding(len(word_index_2)+1,
                            EMBEDDING_DIM,
                            embeddings_initializer=Constant(embedding_matrix),
                            input_length=maxLength,
                            trainable=False)

"""### Strong Baseline: ConvNet Model"""

inp = Input(shape=(maxLength,), dtype='int32')
embedding = embedding_layer(inp)
stacks = []
for kernel_size in [2, 3, 4]:
    conv = Conv1D(64, kernel_size, padding='same', activation='relu', strides=1)(embedding)
    pool = MaxPooling1D(pool_size=3)(conv)
    drop = Dropout(0.4)(pool)
    stacks.append(drop)

merged = Concatenate()(stacks)
flatten = Flatten()(merged)
drop = Dropout(0.4)(flatten)
outp = Dense(8, activation='softmax')(drop)

model_CNN = Model(inputs=inp, outputs=outp)
model_CNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model_CNN.summary()

opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.01)
model_CNN.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[f1_metric,"accuracy"])

model_CNN_run=model_CNN.fit(X_padded,y_train, validation_split=0.2,shuffle=True, batch_size = 32, epochs=300, callbacks=[earlystopping])

acc = model_CNN_run.history['accuracy']
val_acc = model_CNN_run.history['val_accuracy']
loss = model_CNN_run.history['loss']
val_loss = model_CNN_run.history['val_loss']
epochs = range(1, len(acc) + 1)

plt.title('Training and validation accuracy')
plt.plot(epochs, acc, 'blue', label='Training acc')
plt.plot(epochs, val_acc, 'green', label='Validation acc')
plt.legend()

plt.figure()
plt.title('Training and validation loss')
plt.plot(epochs, loss, 'blue', label='Training loss')
plt.plot(epochs, val_loss, 'green', label='Validation loss')
plt.legend()

plt.show()

test_arg = y_val.argmax(axis=1) #argmax gets the class with the highest probability

y_pred = model_CNN.predict(X_val_padded)
pred_arg = y_pred.argmax(axis=1)
co_mat = confusion_matrix(test_arg,pred_arg)

print(co_mat)
print(classification_report(test_arg,pred_arg))

"""### Gridsearch"""

# def build_bilstm(spacial_dropout,bidirectional_dropout, bidirectional_recurrent_dropout):


#   inp = Input(shape=(maxLength,), dtype='int32')
#   x = embedding_layer(inp)
#   x = SpatialDropout1D(spacial_dropout)(x)
#   x = Bidirectional(LSTM(128, return_sequences=True, dropout=bidirectional_dropout, recurrent_dropout=bidirectional_recurrent_dropout))(x)
#   x = Conv1D(64, kernel_size=3)(x)
#   avg_pool = GlobalAveragePooling1D()(x)
#   max_pool = GlobalMaxPooling1D()(x)
#   x = concatenate([avg_pool, max_pool])
#   outp = Dense(8, activation="softmax")(x)

#   BiLSTM = Model(inp, outp)
#   BiLSTM.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=[f1_metric, 'accuracy'])

#   return BiLSTM

# param_grid = {
#     'spacial_dropout': [0.2, 0.5],
#     'bidirectional_dropout': [0.1, 0.3],
#     'bidirectional_recurrent_dropout': [0.1, 0.3],
# }

# timestamp = datetime.now()
# dfGrid = pd.DataFrame(columns = ['spacial_dropout','bidirectional_dropout',
#                                  'bidirectional_recurrent_dropout', 'epochs',
#                                  'train_loss', 'train_f1_metric', 'train_accuracy', 
#                                  'val_loss', 'val_f1_metric', 'val_accuracy'])
# counter = 0

# for spacialdrop in param_grid['spacial_dropout']:
#   for bidirectionaldrop in param_grid['bidirectional_dropout']:
#     for bidirectionalrecdrop in param_grid['bidirectional_recurrent_dropout']:
          
#           print('Training of combination ' + str(counter) + '...')
#           counter += 1

#           # build model with current grid params
#           current_resnet = build_bilstm(spacial_dropout = spacialdrop,
#                                         bidirectional_dropout = bidirectionaldrop,
#                                         bidirectional_recurrent_dropout = bidirectionalrecdrop,)

#           # train model
#           currentHist = current_resnet.fit(x=X_padded,
#                                            y=y_train,
#                                            epochs = 15,
#                                            validation_data = (X_val_padded, y_val),
#                                            callbacks = [earlystopping], 
#                                            verbose = 0)
          

#           # retrieve the index of epoch with maximal val_accuracy -> we wanna save the result of this epoch
#           maxValAC = max(currentHist.history['val_accuracy'])
#           maxValACIndex = currentHist.history['val_accuracy'].index(maxValAC)


#           currentRow = {
#             'spacial_dropout': spacialdrop, 
#             'bidirectional_dropout': bidirectionaldrop, 
#             'bidirectional_recurrent_dropout': bidirectionalrecdrop, 
#             'epochs': maxValACIndex + 1,
#             'train_loss': currentHist.history['loss'][maxValACIndex], 
#             'train_f1_metric': currentHist.history['f1_metric'][maxValACIndex], 
#             'train_accuracy': currentHist.history['accuracy'][maxValACIndex], 
#             'val_loss': currentHist.history['val_loss'][maxValACIndex], 
#             'val_f1_metric': currentHist.history['val_f1_metric'][maxValACIndex], 
#             'val_accuracy': currentHist.history['val_accuracy'][maxValACIndex]
#           }

#           dfGrid = dfGrid.append(currentRow, ignore_index = True)

#           # save the current version of the df so we wouldnt loose it
#           dfGrid.to_csv('grid_' + str(timestamp) + '.csv')
          
# dfGrid

# dfGrid = pd.read_csv('/content/grid_' + str(timestamp) + '.csv')

# dfGrid.to_csv('grid.csv')

# files.download("grid.csv")

# # retrieve the best hyperparameter combination
# bestIndex = dfGrid['val_accuracy'].idxmax()
# bestParams = dfGrid.iloc[bestIndex][['spacial_dropout','bidirectional_dropout',
#                                  'bidirectional_recurrent_dropout']].to_dict()
# optimalNumberOfEpochs = dfGrid.iloc[bestIndex]['epochs']

# print('Best accuracy score found in line: ' + str(bestIndex))
# print('Optimal number of epochs to train: ' + str(optimalNumberOfEpochs))
# print(bestParams)

"""### Bidirectional LSTM model"""

# Bidirectional LSTEM with convolution

inp = Input(shape=(maxLength,), dtype='int32')
x = embedding_layer(inp)
x = SpatialDropout1D(0.2)(x)
x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)
x = Conv1D(64, kernel_size=3)(x)
avg_pool = GlobalAveragePooling1D()(x)
max_pool = GlobalMaxPooling1D()(x)
x = concatenate([avg_pool, max_pool])
outp = Dense(8, activation="softmax")(x)

BiLSTM = Model(inp, outp)
BiLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_metric, 'accuracy'])

BiLSTM.summary()

BiLSTM_run=BiLSTM.fit(x=X_padded,y=y_train, batch_size = 32, epochs=8, validation_data=(X_val_padded, y_val))

acc = BiLSTM_run.history['accuracy']
val_acc = BiLSTM_run.history['val_accuracy']
loss = BiLSTM_run.history['loss']
val_loss = BiLSTM_run.history['val_loss']
epochs = range(1, len(acc) + 1)

plt.title('Training and validation accuracy')
plt.plot(epochs, acc, 'blue', label='Training acc')
plt.plot(epochs, val_acc, 'green', label='Validation acc')
plt.legend()

plt.figure()
plt.title('Training and validation loss')
plt.plot(epochs, loss, 'blue', label='Training loss')
plt.plot(epochs, val_loss, 'green', label='Validation loss')
plt.legend()

plt.show()

test_arg = y_val.argmax(axis=1) #argmax gets the class with the highest probability

y_pred = BiLSTM.predict(X_val_padded)
pred_arg = y_pred.argmax(axis=1)
co_mat = confusion_matrix(test_arg,pred_arg)

print(co_mat)
print(classification_report(test_arg,pred_arg))

prediction = BiLSTM.predict(X_val_padded)


pred_list = []
for i in range(0,len(prediction)):
  max_index_col = np.argmax(prediction[i]) + 1
  pred_list.append(max_index_col)


val_sentence_df = df_val_backup['sentence']

column_vals = pd.Series(pred_list)

val_final_df = pd.concat([val_sentence_df, column_vals], axis=1)

val_final_df.rename(columns={0: 'emotion'}, inplace=True)

val_final_df.to_csv('val_final_df.txt', sep='\t', index=False)

files.download('val_final_df.txt')

f1_metric_precision_recall(y_val, y_pred)

"""### Re-train Model with all available data"""

# Bidirectional LSTEM with convolution

inp = Input(shape=(maxLength,), dtype='int32')
x = embedding_layer_2(inp)
x = SpatialDropout1D(0.2)(x)
x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)
x = Conv1D(64, kernel_size=3)(x)
avg_pool = GlobalAveragePooling1D()(x)
max_pool = GlobalMaxPooling1D()(x)
x = concatenate([avg_pool, max_pool])
outp = Dense(8, activation="softmax")(x)

BiLSTM = Model(inp, outp)
BiLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_metric, 'accuracy'])

BiLSTM.summary()

BiLSTM_run=BiLSTM.fit(x=X_all_train_data_padded,y=y_all_train_data, batch_size = 32, epochs=8)

BiLSTM.save('emotion_labeling_best_model.h5')

files.download('emotion_labeling_best_model.h5')

"""### Predictions on the test dataset"""

prediction = BiLSTM.predict(df_test_padded)

len(prediction)

pred_list = []
for i in range(0,len(prediction)):
  max_index_col = np.argmax(prediction[i]) + 1
  pred_list.append(max_index_col)

test_sentence_df = df_test_backup['sentence']

column_vals = pd.Series(pred_list)

test_final_df = pd.concat([test_sentence_df, column_vals], axis=1)

test_final_df.rename(columns={0: 'emotion'}, inplace=True)

test_final_df

test_final_df.to_csv('test_final_df.txt', sep='\t', index=False)

files.download('test_final_df.txt')