# -*- coding: utf-8 -*-
"""NLP Project - All Models (complete).ipynb

Automatically generated by Colaboratory.


"""

import pandas as pd
from tqdm import tqdm_notebook as tqdm
import nltk
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.stem import SnowballStemmer
from bs4 import BeautifulSoup
import string
import re
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Conv1D, MaxPooling1D, Embedding
from keras import models, layers, metrics, regularizers
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk import PorterStemmer
import string as st
from imblearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.ensemble import BaggingClassifier, VotingClassifier, StackingClassifier, HistGradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from imblearn.under_sampling import CondensedNearestNeighbour, NeighbourhoodCleaningRule, OneSidedSelection, TomekLinks, NearMiss
import gensim
from sklearn import model_selection
from sklearn.model_selection import KFold, cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, \
make_scorer, classification_report, confusion_matrix, f1_score
from sklearn.feature_extraction.text import TfidfVectorizer
from google.colab import files
from sklearn.metrics.pairwise import linear_kernel
pip install zeugma
from sklearn.linear_model import LogisticRegression
from zeugma.embeddings import EmbeddingTransformer

url = 'https://raw.githubusercontent.com/HitGobba/text-mining/main/training_set.txt'
url_1 = 'https://raw.githubusercontent.com/HitGobba/text-mining/main/dev_set.txt'

df_train = pd.read_csv(url, sep="\t")
df_val = pd.read_csv(url_1, sep="\t")

df_train.head()

df_val.head()

"""### Label Counter"""

def label_counter(dataframe, field):
    """
    Function that receives a dataframe and the field whose labels you want to count, and
    returns the amount of examples with those labels in the Pandas dataframe.
    """    
    return dataframe[field].value_counts()

label_counter(df_train, "emotion")

"""### Word Counter"""

def word_counter(text_list):
    """
    Function that receives a list of strings and returns the (absolute) frequency of each word in that list of strings.
    """
    words_in_df = ' '.join(text_list).split()
    
    # Count all words 
    freq = pd.Series(words_in_df).value_counts()
    return freq

word_counter(list(df_train['sentence']))[:25]

"""### NLP Pipeline

### Initial Preprocessing
"""

def remove_punct(text):
    return ("".join([ch for ch in text if ch not in st.punctuation]))

df_train['removed_punc'] = df_train['sentence'].apply(lambda x: remove_punct(x))
df_val['removed_punc'] = df_val['sentence'].apply(lambda x: remove_punct(x))

df_train.head()

def tokenize(text):
    text = re.split('\s+' ,text)
    return [x.lower() for x in text]

df_train['tokens'] = df_train['removed_punc'].apply(lambda msg : tokenize(msg))
df_val['tokens'] = df_val['removed_punc'].apply(lambda msg : tokenize(msg))

df_train.head()

def remove_small_words(text):
    return [x for x in text if len(x) > 3 ]

df_train['larger_tokens'] = df_train['tokens'].apply(lambda x : remove_small_words(x))
df_val['larger_tokens'] = df_val['tokens'].apply(lambda x : remove_small_words(x))

df_train.head()

def remove_stopwords(text):
    return [word for word in text if word not in nltk.corpus.stopwords.words('english')]

nltk.download('stopwords')

df_train['clean_tokens'] = df_train['larger_tokens'].apply(lambda x : remove_stopwords(x))
df_val['clean_tokens'] = df_val['larger_tokens'].apply(lambda x : remove_stopwords(x))

df_train.head()

def stemming(text):
    ps = PorterStemmer()
    return [ps.stem(word) for word in text]

df_train['stem_words'] = df_train['clean_tokens'].apply(lambda wrd: stemming(wrd))
df_val['stem_words'] = df_val['clean_tokens'].apply(lambda wrd: stemming(wrd))

df_train.head()

def lemmatize(text):
    word_net = WordNetLemmatizer()
    return [word_net.lemmatize(word) for word in text]

nltk.download('wordnet')

df_train['lemma_words'] = df_train['clean_tokens'].apply(lambda x : lemmatize(x))
df_val['lemma_words'] = df_val['clean_tokens'].apply(lambda x : lemmatize(x))

df_train.head()

def return_sentences(tokens):
    return " ".join([word for word in tokens])

df_train['clean_text'] = df_train['lemma_words'].apply(lambda x : return_sentences(x))
df_val['clean_text'] = df_val['lemma_words'].apply(lambda x : return_sentences(x))

df_train.head()

df_val.head()

df_train = df_train[['emotion', 'clean_text']]

df_val = df_val[['emotion', 'clean_text']]

df_train.rename(columns = {'clean_text':'sentence'}, inplace=True)
df_val.rename(columns = {'clean_text':'sentence'}, inplace=True)

X_train = df_train["sentence"]
y_train = np.array(df_train["emotion"])

X_val = df_val["sentence"]
y_val = np.array(df_val["emotion"])

model1 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('smote', SMOTE(random_state=9)),
   ('mnb', MultinomialNB())
])

model2 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('smote', SMOTE(random_state=9)),
   ('sgd', SGDClassifier(random_state=9))
])

model3 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('smote', SMOTE(random_state=9)),
   ('lr', LogisticRegression(random_state=9, penalty='l1', solver='saga'))
])

model4 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('smote', SMOTE(random_state=9)),
   ('mlp', MLPClassifier(random_state=9))
])

model5 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('smote', SMOTE(random_state=9)),
   ('gb', GradientBoostingClassifier(random_state=9))
])

model6 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('smote', SMOTE(random_state=9)),
   ('rf', RandomForestClassifier(random_state=9))
])

model7 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('smote', SMOTE(random_state=9)),
   ('dt', DecisionTreeClassifier(random_state=9))
])

model8 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('smote', SMOTE(random_state=9)),
   ('et', ExtraTreesClassifier(random_state=9))
])

model9 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('smote', SMOTE(random_state=9)),
   ('xgb', XGBClassifier(random_state=9))
])

model10 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('smote', SMOTE(random_state=9)),
   ('ab', AdaBoostClassifier(random_state=9))
])

model11 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('smote', SMOTE(random_state=9)),
   ('svc', SVC(random_state=9))
])

model12 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('smote', SMOTE(random_state=9)),
   ('knn', KNeighborsClassifier())
])

model13 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('smote', SMOTE(random_state=9)),
   ('mnb', PassiveAggressiveClassifier(random_state=9))
])

model14 =Pipeline([
  ('vect', CountVectorizer()),
   ('mnb', MultinomialNB())
])

model15 =Pipeline([
  ('vect', CountVectorizer()),
   ('sgd', SGDClassifier(random_state=9))
])

model16 =Pipeline([
  ('vect', CountVectorizer()),
   ('lr', LogisticRegression(random_state=9, penalty='l1', solver='saga'))
])

model17 =Pipeline([
  ('vect', CountVectorizer()),
   ('mlp', MLPClassifier(random_state=9))
])

model18 =Pipeline([
  ('vect', CountVectorizer()),
   ('gb', GradientBoostingClassifier(random_state=9))
])

model19 =Pipeline([
  ('vect', CountVectorizer()),
   ('rf', RandomForestClassifier(random_state=9))
])

model20 =Pipeline([
  ('vect', CountVectorizer()),
   ('dt', DecisionTreeClassifier(random_state=9))
])

model21 =Pipeline([
  ('vect', CountVectorizer()),
   ('et', ExtraTreesClassifier(random_state=9))
])

model22 =Pipeline([
  ('vect', CountVectorizer()),
   ('xgb', XGBClassifier(random_state=9))
])

model23 =Pipeline([
  ('vect', CountVectorizer()),
   ('ab', AdaBoostClassifier(random_state=9))
])

model24 =Pipeline([
  ('vect', CountVectorizer()),
   ('svc', SVC(random_state=9))
])

model25 =Pipeline([
  ('vect', CountVectorizer()),
   ('knn', KNeighborsClassifier())
])

model26 =Pipeline([
  ('vect', CountVectorizer()),
   ('mnb', PassiveAggressiveClassifier(random_state=9))
])




model27 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('cnn', TomekLinks()),
   ('mnb', MultinomialNB())
])

model28 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('cnn', TomekLinks()),
   ('sgd', SGDClassifier(random_state=9))
])

model29 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('cnn', TomekLinks()),
   ('lr', LogisticRegression(random_state=9, penalty='l1', solver='saga'))
])

model30 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('cnn', TomekLinks()),
   ('mlp', MLPClassifier(random_state=9))
])

model31 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('cnn', TomekLinks()),
   ('gb', GradientBoostingClassifier(random_state=9))
])

model32 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('cnn', TomekLinks()),
   ('rf', RandomForestClassifier(random_state=9))
])

model33 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('cnn', TomekLinks()),
   ('dt', DecisionTreeClassifier(random_state=9))
])

model34 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('cnn', TomekLinks()),
   ('et', ExtraTreesClassifier(random_state=9))
])

model35 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('cnn', TomekLinks()),
   ('xgb', XGBClassifier(random_state=9))
])

model36 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('cnn', TomekLinks()),
   ('ab', AdaBoostClassifier(random_state=9))
])

model37 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('cnn', TomekLinks()),
   ('svc', SVC(random_state=9))
])

model38 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('cnn', TomekLinks()),
   ('knn', KNeighborsClassifier())
])

model39 =Pipeline([
  ('vect', CountVectorizer()),
   ('tfidf', TfidfTransformer()),
   ('cnn', TomekLinks()),
   ('mnb', PassiveAggressiveClassifier(random_state=9))
])


model40 =Pipeline([
  ('vect', TfidfVectorizer()),
   ('mnb', MultinomialNB())
])

model41 =Pipeline([
  ('vect', TfidfVectorizer()),
   ('sgd', SGDClassifier(random_state=9))
])

model42 =Pipeline([
  ('vect', TfidfVectorizer()),
   ('lr', LogisticRegression(random_state=9, penalty='l1', solver='saga'))
])

model43 =Pipeline([
  ('vect', TfidfVectorizer()),
   ('mlp', MLPClassifier(random_state=9))
])

model44 =Pipeline([
  ('vect', TfidfVectorizer()),
   ('gb', GradientBoostingClassifier(random_state=9))
])

model45 =Pipeline([
  ('vect', TfidfVectorizer()),
   ('rf', RandomForestClassifier(random_state=9))
])

model46 =Pipeline([
  ('vect', TfidfVectorizer()),
   ('dt', DecisionTreeClassifier(random_state=9))
])

model47 =Pipeline([
  ('vect', TfidfVectorizer()),
   ('et', ExtraTreesClassifier(random_state=9))
])

model48 =Pipeline([
  ('vect', TfidfVectorizer()),
   ('xgb', XGBClassifier(random_state=9))
])

model49 =Pipeline([
  ('vect', TfidfVectorizer()),
   ('ab', AdaBoostClassifier(random_state=9))
])

model50 =Pipeline([
  ('vect', TfidfVectorizer()),
   ('svc', SVC(random_state=9))
])

model51 =Pipeline([
  ('vect', TfidfVectorizer()),
   ('knn', KNeighborsClassifier())
])

model52 =Pipeline([
  ('vect', TfidfVectorizer()),
   ('mnb', PassiveAggressiveClassifier(random_state=9))
])

# model53 =Pipeline([
#    ('smote', SMOTE(random_state=9)),
#    ('mnb', MultinomialNB())
# ])

model54 =Pipeline([
   ('smote', SMOTE(random_state=9)),
   ('sgd', SGDClassifier(random_state=9))
])

model55 =Pipeline([
   ('smote', SMOTE(random_state=9)),
   ('lr', LogisticRegression(random_state=9, penalty='l1', solver='saga'))
])

model56 =Pipeline([
   ('smote', SMOTE(random_state=9)),
   ('mlp', MLPClassifier(random_state=9))
])

model57 =Pipeline([
   ('smote', SMOTE(random_state=9)),
   ('gb', GradientBoostingClassifier(random_state=9))
])

model58 =Pipeline([
   ('smote', SMOTE(random_state=9)),
   ('rf', RandomForestClassifier(random_state=9))
])

model59 =Pipeline([
   ('smote', SMOTE(random_state=9)),
   ('dt', DecisionTreeClassifier(random_state=9))
])

model60 =Pipeline([
   ('smote', SMOTE(random_state=9)),
   ('et', ExtraTreesClassifier(random_state=9))
])

model61 =Pipeline([
   ('smote', SMOTE(random_state=9)),
   ('xgb', XGBClassifier(random_state=9))
])

model62 =Pipeline([
   ('smote', SMOTE(random_state=9)),
   ('ab', AdaBoostClassifier(random_state=9))
])

model63 =Pipeline([
   ('smote', SMOTE(random_state=9)),
   ('svc', SVC(random_state=9))
])

model64 =Pipeline([
   ('smote', SMOTE(random_state=9)),
   ('knn', KNeighborsClassifier())
])

model65 =Pipeline([
   ('smote', SMOTE(random_state=9)),
   ('mnb', PassiveAggressiveClassifier(random_state=9))
])

# model66 =Pipeline([
#    ('mnb', MultinomialNB())
# ])

model67 =Pipeline([
   ('sgd', SGDClassifier(random_state=9))
])

model68 =Pipeline([
   ('lr', LogisticRegression(random_state=9, penalty='l1', solver='saga'))
])

model69 =Pipeline([
   ('mlp', MLPClassifier(random_state=9))
])

model70 =Pipeline([
   ('gb', GradientBoostingClassifier(random_state=9))
])

model71 =Pipeline([
   ('rf', RandomForestClassifier(random_state=9))
])

model72 =Pipeline([
   ('dt', DecisionTreeClassifier(random_state=9))
])

model73 =Pipeline([
   ('et', ExtraTreesClassifier(random_state=9))
])

model74 =Pipeline([
   ('xgb', XGBClassifier(random_state=9))
])

model75 =Pipeline([
   ('ab', AdaBoostClassifier(random_state=9))
])

model76 =Pipeline([
   ('svc', SVC(random_state=9))
])

model77 =Pipeline([
   ('knn', KNeighborsClassifier())
])

model78 =Pipeline([
   ('mnb', PassiveAggressiveClassifier(random_state=9))
])




# model79 =Pipeline([
#    ('cnn', TomekLinks()),
#    ('mnb', MultinomialNB())
# ])

model80 =Pipeline([
   ('cnn', TomekLinks()),
   ('sgd', SGDClassifier(random_state=9))
])

model81 =Pipeline([
   ('cnn', TomekLinks()),
   ('lr', LogisticRegression(random_state=9, penalty='l1', solver='saga'))
])

model82 =Pipeline([
   ('cnn', TomekLinks()),
   ('mlp', MLPClassifier(random_state=9))
])

model83 =Pipeline([
   ('cnn', TomekLinks()),
   ('gb', GradientBoostingClassifier(random_state=9))
])

model84 =Pipeline([
   ('cnn', TomekLinks()),
   ('rf', RandomForestClassifier(random_state=9))
])

model85 =Pipeline([
   ('cnn', TomekLinks()),
   ('dt', DecisionTreeClassifier(random_state=9))
])

model86 =Pipeline([
   ('cnn', TomekLinks()),
   ('et', ExtraTreesClassifier(random_state=9))
])

model87 =Pipeline([
   ('cnn', TomekLinks()),
   ('xgb', XGBClassifier(random_state=9))
])

model88 =Pipeline([
   ('cnn', TomekLinks()),
   ('ab', AdaBoostClassifier(random_state=9))
])

model89 =Pipeline([
   ('cnn', TomekLinks()),
   ('svc', SVC(random_state=9))
])

model90 =Pipeline([
   ('cnn', TomekLinks()),
   ('knn', KNeighborsClassifier())
])

model91 =Pipeline([
   ('cnn', TomekLinks()),
   ('mnb', PassiveAggressiveClassifier(random_state=9))
])

X_train_1 = df_train['sentence'].astype(str).tolist()
X_val_1 = df_val['sentence'].astype(str).tolist()

glove = EmbeddingTransformer('glove')
X_train_glove = glove.transform(X_train_1)

X_val_glove = glove.transform(X_val_1)

def tokenize(text):
    text = re.split('\s+' ,text)
    return [x.lower() for x in str(text)]

df_train['tokens'] = df_train['sentence'].apply(lambda msg : tokenize(msg))

X_train_1 = df_train['sentence'].astype(str).tolist()

X_val_1 = df_val['sentence'].astype(str).tolist()

X_train_1

def score_function(model,pipeline,X_train, y_train, X_val,y_val):

    pipeline.fit(X_train, y_train)
    y_train_pred = pipeline.predict(X_train)
    y_pred = pipeline.predict(X_val)


    
    d = {'model': model, 'accuracy_train': [accuracy_score(y_train, y_train_pred)], 'accuracy_val': [accuracy_score(y_val, y_pred)]}
    return d

model1_scores = pd.DataFrame(score_function('model1', model1, X_train, y_train, X_val,y_val))
model2_scores = pd.DataFrame(score_function('model2', model2, X_train, y_train, X_val,y_val))
model3_scores = pd.DataFrame(score_function('model3', model3, X_train, y_train, X_val,y_val))
model4_scores = pd.DataFrame(score_function('model4', model4, X_train, y_train, X_val,y_val))
model5_scores = pd.DataFrame(score_function('model5', model5, X_train, y_train, X_val,y_val))
model6_scores = pd.DataFrame(score_function('model6', model6, X_train, y_train, X_val,y_val))
model7_scores = pd.DataFrame(score_function('model7', model7, X_train, y_train, X_val,y_val))
model8_scores = pd.DataFrame(score_function('model8', model8, X_train, y_train, X_val,y_val))
model9_scores = pd.DataFrame(score_function('model9', model9, X_train, y_train, X_val,y_val))
model10_scores = pd.DataFrame(score_function('model10', model10, X_train, y_train, X_val,y_val))
model11_scores = pd.DataFrame(score_function('model11', model11, X_train, y_train, X_val,y_val))
model12_scores = pd.DataFrame(score_function('model12', model12, X_train, y_train, X_val,y_val))
model13_scores = pd.DataFrame(score_function('model13', model13, X_train, y_train, X_val,y_val))
model14_scores = pd.DataFrame(score_function('model14', model14, X_train, y_train, X_val,y_val))
model15_scores = pd.DataFrame(score_function('model15', model15, X_train, y_train, X_val,y_val))
model16_scores = pd.DataFrame(score_function('model16', model16, X_train, y_train, X_val,y_val))
model17_scores = pd.DataFrame(score_function('model17', model17, X_train, y_train, X_val,y_val))
model18_scores = pd.DataFrame(score_function('model18', model18, X_train, y_train, X_val,y_val))
model19_scores = pd.DataFrame(score_function('model19', model19, X_train, y_train, X_val,y_val))
model20_scores = pd.DataFrame(score_function('model20', model20, X_train, y_train, X_val,y_val))
model21_scores = pd.DataFrame(score_function('model21', model21, X_train, y_train, X_val,y_val))
model22_scores = pd.DataFrame(score_function('model22', model22, X_train, y_train, X_val,y_val))
model23_scores = pd.DataFrame(score_function('model23', model23, X_train, y_train, X_val,y_val))
model24_scores = pd.DataFrame(score_function('model24', model24, X_train, y_train, X_val,y_val))
model25_scores = pd.DataFrame(score_function('model25', model25, X_train, y_train, X_val,y_val))
model26_scores = pd.DataFrame(score_function('model26', model26, X_train, y_train, X_val,y_val))
model27_scores = pd.DataFrame(score_function('model27', model27, X_train, y_train, X_val,y_val))
model28_scores = pd.DataFrame(score_function('model28', model28, X_train, y_train, X_val,y_val))
model29_scores = pd.DataFrame(score_function('model29', model29, X_train, y_train, X_val,y_val))
model30_scores = pd.DataFrame(score_function('model30', model30, X_train, y_train, X_val,y_val))
model31_scores = pd.DataFrame(score_function('model31', model31, X_train, y_train, X_val,y_val))
model32_scores = pd.DataFrame(score_function('model32', model32, X_train, y_train, X_val,y_val))
model33_scores = pd.DataFrame(score_function('model33', model33, X_train, y_train, X_val,y_val))
model34_scores = pd.DataFrame(score_function('model34', model34, X_train, y_train, X_val,y_val))
model35_scores = pd.DataFrame(score_function('model35', model35, X_train, y_train, X_val,y_val))
model36_scores = pd.DataFrame(score_function('model36', model36, X_train, y_train, X_val,y_val))
model37_scores = pd.DataFrame(score_function('model37', model37, X_train, y_train, X_val,y_val))
model38_scores = pd.DataFrame(score_function('model38', model38, X_train, y_train, X_val,y_val))
model39_scores = pd.DataFrame(score_function('model39', model39, X_train, y_train, X_val,y_val))
model40_scores = pd.DataFrame(score_function('model40', model40, X_train, y_train, X_val,y_val))
model41_scores = pd.DataFrame(score_function('model41', model41, X_train, y_train, X_val,y_val))
model42_scores = pd.DataFrame(score_function('model42', model42, X_train, y_train, X_val,y_val))
model43_scores = pd.DataFrame(score_function('model43', model43, X_train, y_train, X_val,y_val))
model44_scores = pd.DataFrame(score_function('model44', model44, X_train, y_train, X_val,y_val))
model45_scores = pd.DataFrame(score_function('model45', model45, X_train, y_train, X_val,y_val))
model46_scores = pd.DataFrame(score_function('model46', model46, X_train, y_train, X_val,y_val))
model47_scores = pd.DataFrame(score_function('model47', model47, X_train, y_train, X_val,y_val))
model48_scores = pd.DataFrame(score_function('model48', model48, X_train, y_train, X_val,y_val))
model49_scores = pd.DataFrame(score_function('model49', model49, X_train, y_train, X_val,y_val))
model50_scores = pd.DataFrame(score_function('model50', model50, X_train, y_train, X_val,y_val))
model51_scores = pd.DataFrame(score_function('model51', model51, X_train, y_train, X_val,y_val))
model52_scores = pd.DataFrame(score_function('model52', model52, X_train, y_train, X_val,y_val))
#model53_scores = pd.DataFrame(score_function('model53', model53, X_train_glove, y_train, X_val_glove,y_val))
model54_scores = pd.DataFrame(score_function('model54', model54, X_train_glove, y_train, X_val_glove,y_val))
model55_scores = pd.DataFrame(score_function('model55', model55, X_train_glove, y_train, X_val_glove,y_val))
model56_scores = pd.DataFrame(score_function('model56', model56, X_train_glove, y_train, X_val_glove,y_val))
model57_scores = pd.DataFrame(score_function('model57', model57, X_train_glove, y_train, X_val_glove,y_val))
model58_scores = pd.DataFrame(score_function('model58', model58, X_train_glove, y_train, X_val_glove,y_val))
model59_scores = pd.DataFrame(score_function('model59', model59, X_train_glove, y_train, X_val_glove,y_val))
model60_scores = pd.DataFrame(score_function('model60', model60, X_train_glove, y_train, X_val_glove,y_val))
model61_scores = pd.DataFrame(score_function('model61', model61, X_train_glove, y_train, X_val_glove,y_val))
model62_scores = pd.DataFrame(score_function('model62', model62, X_train_glove, y_train, X_val_glove,y_val))
model63_scores = pd.DataFrame(score_function('model63', model63, X_train_glove, y_train, X_val_glove,y_val))
model64_scores = pd.DataFrame(score_function('model64', model64, X_train_glove, y_train, X_val_glove,y_val))
model65_scores = pd.DataFrame(score_function('model65', model65, X_train_glove, y_train, X_val_glove,y_val))
#model66_scores = pd.DataFrame(score_function('model66', model66, X_train_glove, y_train, X_val_glove,y_val))
model67_scores = pd.DataFrame(score_function('model67', model67, X_train_glove, y_train, X_val_glove,y_val))
model68_scores = pd.DataFrame(score_function('model68', model68, X_train_glove, y_train, X_val_glove,y_val))
model69_scores = pd.DataFrame(score_function('model69', model69, X_train_glove, y_train, X_val_glove,y_val))
model70_scores = pd.DataFrame(score_function('model70', model70, X_train_glove, y_train, X_val_glove,y_val))
model71_scores = pd.DataFrame(score_function('model71', model71, X_train_glove, y_train, X_val_glove,y_val))
model72_scores = pd.DataFrame(score_function('model72', model72, X_train_glove, y_train, X_val_glove,y_val))
model73_scores = pd.DataFrame(score_function('model73', model73, X_train_glove, y_train, X_val_glove,y_val))
model74_scores = pd.DataFrame(score_function('model74', model74, X_train_glove, y_train, X_val_glove,y_val))
model75_scores = pd.DataFrame(score_function('model75', model75, X_train_glove, y_train, X_val_glove,y_val))
model76_scores = pd.DataFrame(score_function('model76', model76, X_train_glove, y_train, X_val_glove,y_val))
model77_scores = pd.DataFrame(score_function('model77', model77, X_train_glove, y_train, X_val_glove,y_val))
model78_scores = pd.DataFrame(score_function('model78', model78, X_train_glove, y_train, X_val_glove,y_val))
#model79_scores = pd.DataFrame(score_function('model79', model79, X_train_glove, y_train, X_val_glove,y_val))
model80_scores = pd.DataFrame(score_function('model80', model80, X_train_glove, y_train, X_val_glove,y_val))
model81_scores = pd.DataFrame(score_function('model81', model81, X_train_glove, y_train, X_val_glove,y_val))
model82_scores = pd.DataFrame(score_function('model82', model82, X_train_glove, y_train, X_val_glove,y_val))
model83_scores = pd.DataFrame(score_function('model83', model83, X_train_glove, y_train, X_val_glove,y_val))
model84_scores = pd.DataFrame(score_function('model84', model84, X_train_glove, y_train, X_val_glove,y_val))
model85_scores = pd.DataFrame(score_function('model85', model85, X_train_glove, y_train, X_val_glove,y_val))
model86_scores = pd.DataFrame(score_function('model86', model86, X_train_glove, y_train, X_val_glove,y_val))
model87_scores = pd.DataFrame(score_function('model87', model87, X_train_glove, y_train, X_val_glove,y_val))
model88_scores = pd.DataFrame(score_function('model88', model88, X_train_glove, y_train, X_val_glove,y_val))
model89_scores = pd.DataFrame(score_function('model89', model89, X_train_glove, y_train, X_val_glove,y_val))
model90_scores = pd.DataFrame(score_function('model90', model90, X_train_glove, y_train, X_val_glove,y_val))
model91_scores = pd.DataFrame(score_function('model91', model91, X_train_glove, y_train, X_val_glove,y_val))

scores_df = pd.concat([
                    pd.DataFrame(model1_scores),
                    pd.DataFrame(model2_scores),
                    pd.DataFrame(model3_scores),
                    pd.DataFrame(model4_scores),
                    pd.DataFrame(model5_scores),
                    pd.DataFrame(model6_scores),
                    pd.DataFrame(model7_scores),
                    pd.DataFrame(model8_scores),
                    pd.DataFrame(model9_scores),
                    pd.DataFrame(model10_scores),
                    pd.DataFrame(model11_scores),
                    pd.DataFrame(model12_scores),
                    pd.DataFrame(model13_scores),
                    pd.DataFrame(model14_scores),
                    pd.DataFrame(model15_scores),
                    pd.DataFrame(model16_scores),
                    pd.DataFrame(model17_scores),
                    pd.DataFrame(model18_scores),
                    pd.DataFrame(model19_scores),
                    pd.DataFrame(model20_scores),
                    pd.DataFrame(model21_scores),
                    pd.DataFrame(model22_scores),
                    pd.DataFrame(model23_scores),
                    pd.DataFrame(model24_scores),
                    pd.DataFrame(model25_scores),
                    pd.DataFrame(model26_scores),
                    pd.DataFrame(model27_scores),
                    pd.DataFrame(model28_scores),
                    pd.DataFrame(model29_scores),
                    pd.DataFrame(model30_scores),
                    pd.DataFrame(model31_scores),
                    pd.DataFrame(model32_scores),
                    pd.DataFrame(model33_scores),
                    pd.DataFrame(model34_scores),
                    pd.DataFrame(model35_scores),
                    pd.DataFrame(model36_scores),
                    pd.DataFrame(model37_scores),
                    pd.DataFrame(model38_scores),
                    pd.DataFrame(model39_scores),
                    pd.DataFrame(model40_scores),
                    pd.DataFrame(model41_scores),
                    pd.DataFrame(model42_scores),
                    pd.DataFrame(model43_scores),
                    pd.DataFrame(model44_scores),
                    pd.DataFrame(model45_scores),
                    pd.DataFrame(model46_scores),
                    pd.DataFrame(model47_scores),
                    pd.DataFrame(model48_scores),
                    pd.DataFrame(model49_scores),
                    pd.DataFrame(model50_scores),
                    pd.DataFrame(model51_scores),
                    pd.DataFrame(model52_scores),
                    #pd.DataFrame(model53_scores),
                    pd.DataFrame(model54_scores),
                    pd.DataFrame(model55_scores),
                    pd.DataFrame(model56_scores),
                    pd.DataFrame(model57_scores),
                    pd.DataFrame(model58_scores),
                    pd.DataFrame(model59_scores),
                    pd.DataFrame(model60_scores),
                    pd.DataFrame(model61_scores),
                    pd.DataFrame(model62_scores),
                    pd.DataFrame(model63_scores),
                    pd.DataFrame(model64_scores),
                    pd.DataFrame(model65_scores),
                    #pd.DataFrame(model66_scores),
                    pd.DataFrame(model67_scores),
                    pd.DataFrame(model68_scores),
                    pd.DataFrame(model69_scores),
                    pd.DataFrame(model70_scores),
                    pd.DataFrame(model71_scores),
                    pd.DataFrame(model72_scores),
                    pd.DataFrame(model73_scores),
                    pd.DataFrame(model74_scores),
                    pd.DataFrame(model75_scores),
                    pd.DataFrame(model76_scores),
                    pd.DataFrame(model77_scores),
                    pd.DataFrame(model78_scores),
                    #pd.DataFrame(model79_scores),
                    pd.DataFrame(model80_scores),
                    pd.DataFrame(model81_scores),
                    pd.DataFrame(model82_scores),
                    pd.DataFrame(model83_scores),
                    pd.DataFrame(model84_scores),
                    pd.DataFrame(model85_scores),
                    pd.DataFrame(model86_scores),
                    pd.DataFrame(model87_scores),
                    pd.DataFrame(model88_scores),
                    pd.DataFrame(model89_scores),
                    pd.DataFrame(model90_scores),
                    pd.DataFrame(model91_scores)],
                    axis=0)

scores_df.to_csv('scores.csv')

files.download("scores.csv")

"""### Training the model"""

def score(y_val,y_pred):
    print('Micro f1 score:', f1_score(y_val, y_pred, average='micro'))
    print('\nResults on the data set:')
    labels = {"Anger": 1, "Anticipation": 2, "Disgust": 3, "Fear": 4, "Joy": 5, "Sadness":6, "Surprise":7, "Trust":8}
    print(classification_report(y_true = y_val, y_pred = y_pred, target_names=labels))

labels = {"Anger": 1, "Anticipation": 2, "Disgust": 3, "Fear": 4, "Joy": 5, "Sadness":6, "Surprise":7, "Trust":8}

def plot_cm(confusion_matrix : np.array, 
            classnames : list):
    """
    Function that creates a confusion matrix plot using the Wikipedia convention for the axis. 
    :param confusion_matrix: confusion matrix that will be plotted
    :param classnames: labels of the classes
    
    Returns:
        - Plot of the Confusion Matrix
    """
    
    confusionmatrix = confusion_matrix
    class_names = classnames             

    fig, ax = plt.subplots()
    im = plt.imshow(confusionmatrix, cmap=plt.cm.RdYlBu)
    plt.colorbar()

    # We want to show all ticks...
    ax.set_xticks(np.arange(len(class_names)))
    ax.set_yticks(np.arange(len(class_names)))
    # ... and label them with the respective list entries
    ax.set_xticklabels(class_names)
    ax.set_yticklabels(class_names)

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    for i in range(len(class_names)):
        for j in range(len(class_names)):
            text = ax.text(j, i, confusionmatrix[i, j],
                           ha="center", va="center", color="w")

    ax.set_title("Confusion Matrix")
    plt.xlabel('Targets')
    plt.ylabel('Predictions')
    plt.ylim(top=len(class_names)-0.5)  # adjust the top leaving bottom unchanged
    plt.ylim(bottom=-0.5)  # adjust the bottom leaving top unchanged
    return plt.show()

"""### Deep Learning and Transfer Learning"""

import pandas as pd
from tqdm import tqdm_notebook as tqdm
import nltk
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.stem import SnowballStemmer
from bs4 import BeautifulSoup
import string
import re
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, Bidirectional, BatchNormalization, Conv1D, MaxPooling1D, Embedding, Input, Activation, LSTM, GRU, GlobalMaxPooling1D
from keras import models, layers, metrics, regularizers
import string as st
from nltk import PorterStemmer
import string
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from keras import models, layers, metrics, regularizers
import keras
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.models import Sequential, load_model
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras import backend as K
from keras.callbacks import EarlyStopping
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical
from keras.initializers import Constant
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Layer
from keras import initializers, constraints

url = 'https://raw.githubusercontent.com/HitGobba/text-mining/main/training_set.txt'
url_1 = 'https://raw.githubusercontent.com/HitGobba/text-mining/main/dev_set.txt'

df_train = pd.read_csv(url, sep="\t")
df_val = pd.read_csv(url_1, sep="\t")

df_train.head()

df_val.head()

"""### Metrics computing"""

def f1_metric(y_true, y_pred):

    def recall(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())
        return recall

    def precision(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())
        return precision

    precision = precision(y_true, y_pred)
    recall = recall(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

def confusion_matrix(data_set, predictions):
    print('Confusion matrix')
    return cm(data_set, predictions)

def classification_report(data_set, predictions):
    print('Classification Report')
    target_names = ["Anger", "Anticipation", "Disgust", "Fear", "Joy", "Sadness", "Surprise", "Trust"]
    print(cr(data_set, predictions, target_names=target_names))

"""### Callbacks"""

callbacks_list = [
    keras.callbacks.EarlyStopping( # Once triggered, stops the training (if the val_f1 decreases 3 epochs in a row).
        monitor='val_accuracy',
        patience=3),
    keras.callbacks.ModelCheckpoint( # Saves the best model observed during training (according to val_loss).
        filepath = 'cnn_model_file',
        monitor='val_loss',
        save_best_only=True)
]

earlystopping = EarlyStopping(monitor ="val_accuracy",  
                                        mode ="max", 
                                        patience =3,
                                        verbose=1,  
                                        restore_best_weights = True)

my_callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=3),
    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),
    tf.keras.callbacks.TensorBoard(log_dir='./logs'),
]

"""### Label Counter"""

def label_counter(dataframe, field):
    """
    Function that receives a dataframe and the field whose labels you want to count, and
    returns the amount of examples with those labels in the Pandas dataframe.
    """    
    return dataframe[field].value_counts()

label_counter(df_train, "emotion")

"""### Word Counter"""

def word_counter(text_list):
    """
    Function that receives a list of strings and returns the (absolute) frequency of each word in that list of strings.
    """
    words_in_df = ' '.join(text_list).split()
    
    # Count all words 
    freq = pd.Series(words_in_df).value_counts()
    return freq

word_counter(list(df_train['sentence']))[:25]

"""### NLP Pipeline

### Initial Preprocessing
"""

def remove_punct(text):
    return ("".join([ch for ch in text if ch not in st.punctuation]))

df_train['removed_punc'] = df_train['sentence'].apply(lambda x: remove_punct(x))
df_val['removed_punc'] = df_val['sentence'].apply(lambda x: remove_punct(x))

df_train.head()

def tokenize(text):
    text = re.split('\s+' ,text)
    return [x.lower() for x in text]

df_train['tokens'] = df_train['removed_punc'].apply(lambda msg : tokenize(msg))
df_val['tokens'] = df_val['removed_punc'].apply(lambda msg : tokenize(msg))

df_train.head()

def remove_small_words(text):
    return [x for x in text if len(x) > 0 ]

df_train['larger_tokens'] = df_train['tokens'].apply(lambda x : remove_small_words(x))
df_val['larger_tokens'] = df_val['tokens'].apply(lambda x : remove_small_words(x))

df_train.head()

def remove_stopwords(text):
    return [word for word in text if word not in nltk.corpus.stopwords.words('english')]

nltk.download('stopwords')

df_train['clean_tokens'] = df_train['larger_tokens'].apply(lambda x : remove_stopwords(x))
df_val['clean_tokens'] = df_val['larger_tokens'].apply(lambda x : remove_stopwords(x))

df_train.head()

def stemming(text):
    ps = PorterStemmer()
    return [ps.stem(word) for word in text]

df_train['stem_words'] = df_train['clean_tokens'].apply(lambda wrd: stemming(wrd))
df_val['stem_words'] = df_val['clean_tokens'].apply(lambda wrd: stemming(wrd))

df_train.head()

def lemmatize(text):
    word_net = WordNetLemmatizer()
    return [word_net.lemmatize(word) for word in text]

nltk.download('wordnet')

df_train['lemma_words'] = df_train['clean_tokens'].apply(lambda x : lemmatize(x))
df_val['lemma_words'] = df_val['clean_tokens'].apply(lambda x : lemmatize(x))

df_train.head()

def return_sentences(tokens):
    return " ".join([word for word in tokens])

df_train['clean_text'] = df_train['lemma_words'].apply(lambda x : return_sentences(x))
df_val['clean_text'] = df_val['lemma_words'].apply(lambda x : return_sentences(x))

df_train.head()

df_val.head()

df_train = df_train[['emotion', 'clean_text']]

df_val = df_val[['emotion', 'clean_text']]

df_train.rename(columns = {'clean_text':'sentence'}, inplace=True)
df_val.rename(columns = {'clean_text':'sentence'}, inplace=True)

X_train = df_train['sentence']
y_train = df_train['emotion']

X_val = df_val['sentence']
y_val = df_val['emotion']

# tokenizing
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_val = tokenizer.texts_to_sequences(X_val)

maxLength = 60

word_index = tokenizer.word_index
len(word_index)

X_padded = pad_sequences(X_train,padding='post', maxlen=maxLength)

X_val_padded = pad_sequences(X_val,padding='post', maxlen=maxLength)

sanityCheckIndex={v: k for k, v in tokenizer.word_index.items()}
print(X_train[500])
print(' '.join([sanityCheckIndex[wordIndex] for wordIndex in X_train[500]]))
print(X_padded[500][0])
print(X_padded[500][-1])
print(' '.join([sanityCheckIndex[wordIndex] for wordIndex in X_padded[500] if wordIndex!=0 ]))

tokenizer.word_index.items()

# Label encoding
enc = LabelEncoder()
y_train=enc.fit_transform(y_train)
y_train=to_categorical(y_train)

y_val=enc.fit_transform(y_val)
y_val=to_categorical(y_val)

!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip glove*.zip.2

!ls
!pwd

# Using Glove for word embeddings
word_index = tokenizer.word_index
EMBEDDING_DIM = 100

embeddings_index = {}

#f = open('/content/drive/My Drive/globe.6B/glove.6B.100d.txt')
f = open('glove.6B.100d.txt', encoding='utf-8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s unique tokens.' % len(word_index))
print('Total %s word vectors.' % len(embeddings_index))

word_index = tokenizer.word_index
embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM)) 
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)  
    if embedding_vector is not None: 
        embedding_matrix[i] = embedding_vector

embedding_matrix.shape

# predefining the embedding layer
embedding_layer = Embedding(len(word_index)+1,
                            EMBEDDING_DIM,
                            embeddings_initializer=Constant(embedding_matrix),
                            input_length=maxLength,
                            trainable=False)

"""### RNN"""

#RNN with LSTM
model = Sequential()

model.add(embedding_layer)
model.add(LSTM(300, dropout=0.25, recurrent_dropout=0.25))
model.add(Dropout(0.4))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.4))
model.add(BatchNormalization())
model.add(Dense(8, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

modelrun=model.fit(X_padded,y_train, validation_split=0.2,shuffle=True, batch_size = 128, epochs=20)

acc = modelrun.history['accuracy']
val_acc = modelrun.history['val_accuracy']
loss = modelrun.history['loss']
val_loss = modelrun.history['val_loss']
epochs = range(1, len(acc) + 1)

plt.title('Training and validation accuracy')
plt.plot(epochs, acc, 'blue', label='Training acc')
plt.plot(epochs, val_acc, 'green', label='Validation acc')
plt.legend()

plt.figure()
plt.title('Training and validation loss')
plt.plot(epochs, loss, 'blue', label='Training loss')
plt.plot(epochs, val_loss, 'green', label='Validation loss')
plt.legend()

plt.show()

"""### ConvNet Model"""

inp = Input(shape=(maxLength,), dtype='int32')
embedding = embedding_layer(inp)
stacks = []
for kernel_size in [2, 3, 4]:
    conv = Conv1D(64, kernel_size, padding='same', activation='relu', strides=1)(embedding)
    pool = MaxPooling1D(pool_size=3)(conv)
    drop = Dropout(0.4)(pool)
    stacks.append(drop)

merged = Concatenate()(stacks)
flatten = Flatten()(merged)
drop = Dropout(0.4)(flatten)
outp = Dense(8, activation='softmax')(drop)

model_CNN = Model(inputs=inp, outputs=outp)
model_CNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model_CNN.summary()

opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.01)
model_CNN.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[f1_metric,"accuracy"])

model_CNN_run=model_CNN.fit(X_padded,y_train, validation_split=0.2,shuffle=True, batch_size = 32, epochs=300)

acc = model_CNN_run.history['accuracy']
val_acc = model_CNN_run.history['val_accuracy']
loss = model_CNN_run.history['loss']
val_loss = model_CNN_run.history['val_loss']
epochs = range(1, len(acc) + 1)

plt.title('Training and validation accuracy')
plt.plot(epochs, acc, 'blue', label='Training acc')
plt.plot(epochs, val_acc, 'green', label='Validation acc')
plt.legend()

plt.figure()
plt.title('Training and validation loss')
plt.plot(epochs, loss, 'blue', label='Training loss')
plt.plot(epochs, val_loss, 'green', label='Validation loss')
plt.legend()

plt.show()

"""### CNN + Bidirectional GRU model"""

X_input = Input(shape = (maxLength,),dtype='int32')
    

embedded_sequences = embedding_layer(X_input)    

X = Conv1D(filters=64,kernel_size=4,strides=1)(embedded_sequences)                              
X = BatchNormalization()(X)                               
X = Activation("relu")(X)                               
X = Dropout(rate=0.4)(X)                            


X = Bidirectional(GRU(units=128, return_sequences = True,dropout=0.1, recurrent_dropout=0.1))(X)                             
                            
X = BatchNormalization()(X)                               
    
   
X = GRU(units=256, return_sequences = True)(X)                                  
X = Dropout(rate=0.4)(X)                               
X = BatchNormalization()(X)                             
X = Dropout(rate=0.4)(X)                                
    
max_pool = GlobalMaxPooling1D()(X)

X = (Dense(8, activation = "softmax"))(max_pool)



model_GRU = Model(inputs = X_input, outputs = X)
model_GRU.summary()

opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.01)
model_GRU.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[f1_metric, "accuracy"])

model_GRU_run=model_GRU.fit(X_padded,y_train, validation_split=0.2,shuffle=True, batch_size = 32, epochs=100, callbacks=[earlystopping])

acc = model_GRU_run.history['accuracy']
val_acc = model_GRU_run.history['val_accuracy']
loss = model_GRU_run.history['loss']
val_loss = model_GRU_run.history['val_loss']
epochs = range(1, len(acc) + 1)

plt.title('Training and validation accuracy')
plt.plot(epochs, acc, 'blue', label='Training acc')
plt.plot(epochs, val_acc, 'green', label='Validation acc')
plt.legend()

plt.figure()
plt.title('Training and validation loss')
plt.plot(epochs, loss, 'blue', label='Training loss')
plt.plot(epochs, val_loss, 'green', label='Validation loss')
plt.legend()

plt.show()

"""### Bidirectional LSTM model"""

# Bidirectional LSTEM with convolution

inp = Input(shape=(maxLength,), dtype='int32')
x = embedding_layer(inp)
x = SpatialDropout1D(0.2)(x)
x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)
x = Conv1D(64, kernel_size=3)(x)
avg_pool = GlobalAveragePooling1D()(x)
max_pool = GlobalMaxPooling1D()(x)
x = concatenate([avg_pool, max_pool])
outp = Dense(8, activation="softmax")(x)

BiLSTM = Model(inp, outp)
BiLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_metric, 'accuracy'])

BiLSTM.summary()

BiLSTM_run=BiLSTM.fit(x=X_padded,y=y_train, batch_size = 32, epochs=15, validation_data=(X_val_padded, y_val),callbacks=[earlystopping])

acc = BiLSTM_run.history['accuracy']
val_acc = BiLSTM_run.history['val_accuracy']
loss = BiLSTM_run.history['loss']
val_loss = BiLSTM_run.history['val_loss']
epochs = range(1, len(acc) + 1)

plt.title('Training and validation accuracy')
plt.plot(epochs, acc, 'blue', label='Training acc')
plt.plot(epochs, val_acc, 'green', label='Validation acc')
plt.legend()

plt.figure()
plt.title('Training and validation loss')
plt.plot(epochs, loss, 'blue', label='Training loss')
plt.plot(epochs, val_loss, 'green', label='Validation loss')
plt.legend()

plt.show()

test_arg = y_val.argmax(axis=1) #argmax gets the class with the highest probability

y_pred = BiLSTM.predict(X_val_padded)
pred_arg = y_pred.argmax(axis=1)
co_mat = confusion_matrix(test_arg,pred_arg)

print(co_mat)
print(classification_report(test_arg,pred_arg))

def build_bilstm(spacial_dropout,bidirectional_dropout, bidirectional_recurrent_dropout):


  inp = Input(shape=(maxLength,), dtype='int32')
  x = embedding_layer(inp)
  x = SpatialDropout1D(spacial_dropout)(x)
  x = Bidirectional(LSTM(128, return_sequences=True, dropout=bidirectional_dropout, recurrent_dropout=bidirectional_recurrent_dropout))(x)
  x = Conv1D(64, kernel_size=3)(x)
  avg_pool = GlobalAveragePooling1D()(x)
  max_pool = GlobalMaxPooling1D()(x)
  x = concatenate([avg_pool, max_pool])
  outp = Dense(8, activation="softmax")(x)

  BiLSTM = Model(inp, outp)
  BiLSTM.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=[f1_metric, 'accuracy'])

  return BiLSTM

param_grid = {
    'spacial_dropout': [0.2, 0.5],
    'bidirectional_dropout': [0.1, 0.3],
    'bidirectional_recurrent_dropout': [0.1, 0.3],
}

timestamp = datetime.now()
dfGrid = pd.DataFrame(columns = ['spacial_dropout','bidirectional_dropout',
                                 'bidirectional_recurrent_dropout', 'epochs',
                                 'train_loss', 'train_f1_metric', 'train_accuracy', 
                                 'val_loss', 'val_f1_metric', 'val_accuracy'])
counter = 0

for spacialdrop in param_grid['spacial_dropout']:
  for bidirectionaldrop in param_grid['bidirectional_dropout']:
    for bidirectionalrecdrop in param_grid['bidirectional_recurrent_dropout']:
          
          print('Training of combination ' + str(counter) + '...')
          counter += 1

          # build model with current grid params
          current_resnet = build_bilstm(spacial_dropout = spacialdrop,
                                        bidirectional_dropout = bidirectionaldrop,
                                        bidirectional_recurrent_dropout = bidirectionalrecdrop,)

          # train model
          currentHist = current_resnet.fit(x=X_padded,
                                           y=y_train,
                                           epochs = 15,
                                           validation_data = (X_val_padded, y_val),
                                           callbacks = [earlystopping], 
                                           verbose = 0)
          

          # retrieve the index of epoch with maximal val_accuracy -> we wanna save the result of this epoch
          maxValAC = max(currentHist.history['val_accuracy'])
          maxValACIndex = currentHist.history['val_accuracy'].index(maxValAC)


          currentRow = {
            'spacial_dropout': spacialdrop, 
            'bidirectional_dropout': bidirectionaldrop, 
            'bidirectional_recurrent_dropout': bidirectionalrecdrop, 
            'epochs': maxValACIndex + 1,
            'train_loss': currentHist.history['loss'][maxValACIndex], 
            'train_f1_metric': currentHist.history['f1_metric'][maxValACIndex], 
            'train_accuracy': currentHist.history['accuracy'][maxValACIndex], 
            'val_loss': currentHist.history['val_loss'][maxValACIndex], 
            'val_f1_metric': currentHist.history['val_f1_metric'][maxValACIndex], 
            'val_accuracy': currentHist.history['val_accuracy'][maxValACIndex]
          }

          dfGrid = dfGrid.append(currentRow, ignore_index = True)

          # save the current version of the df so we wouldnt loose it
          dfGrid.to_csv('grid_' + str(timestamp) + '.csv')
          
dfGrid

dfGrid = pd.read_csv('/content/grid_' + str(timestamp) + '.csv')

dfGrid.to_csv('grid.csv')

files.download("grid.csv")

# retrieve the best hyperparameter combination
bestIndex = dfGrid['val_accuracy'].idxmax()
bestParams = dfGrid.iloc[bestIndex][['spacial_dropout','bidirectional_dropout',
                                 'bidirectional_recurrent_dropout']].to_dict()
optimalNumberOfEpochs = dfGrid.iloc[bestIndex]['epochs']

print('Best accuracy score found in line: ' + str(bestIndex))
print('Optimal number of epochs to train: ' + str(optimalNumberOfEpochs))
print(bestParams)

"""### LSTM with Attention"""

#LSTM with Attention

class Attention(Layer):
    def __init__(self, step_dim,
                 W_regularizer=None, b_regularizer=None,
                 W_constraint=None, b_constraint=None,
                 bias=True, **kwargs):
        self.supports_masking = True
        self.init = initializers.get('glorot_uniform')

        self.W_regularizer = regularizers.get(W_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        self.step_dim = step_dim
        self.features_dim = 0
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        assert len(input_shape) == 3

        self.W = self.add_weight(shape=(input_shape[-1],),
                                 initializer=self.init,
                                 name='{}_W'.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        self.features_dim = input_shape[-1]

        if self.bias:
            self.b = self.add_weight(shape=(input_shape[1],),
                                     initializer='zero',
                                     name='{}_b'.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)
        else:
            self.b = None

        self.built = True

    def compute_mask(self, input, input_mask=None):
        return None

    def call(self, x, mask=None):
        features_dim = self.features_dim
        step_dim = self.step_dim

        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))

        if self.bias:
            eij += self.b

        eij = K.tanh(eij)

        a = K.exp(eij)


        if mask is not None:
            a *= K.cast(mask, K.floatx())
        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())

        a = K.expand_dims(a)
        weighted_input = x * a
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0],  self.features_dim

    

lstm_layer = LSTM(300, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)

inp = Input(shape=(maxLength,), dtype='int32')
embedding= embedding_layer(inp)
x = lstm_layer(embedding)
x = Dropout(0.25)(x)
merged = Attention(maxLength)(x)
merged = Dense(256, activation='relu')(merged)
merged = Dropout(0.25)(merged)
merged = BatchNormalization()(merged)
outp = Dense(8, activation='softmax')(merged)

AttentionLSTM = Model(inputs=inp, outputs=outp)
AttentionLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_metric, 'accuracy'])

AttentionLSTM.summary()

AttentionLSTM_run=AttentionLSTM.fit(X_padded,y_train, validation_split=0.2,shuffle=True, batch_size = 32, epochs=30, callbacks=[earlystopping])

acc = AttentionLSTM_run.history['accuracy']
val_acc = AttentionLSTM_run.history['val_accuracy']
loss = AttentionLSTM_run.history['loss']
val_loss = AttentionLSTM_run.history['val_loss']
epochs = range(1, len(acc) + 1)

plt.title('Training and validation accuracy')
plt.plot(epochs, acc, 'blue', label='Training acc')
plt.plot(epochs, val_acc, 'green', label='Validation acc')
plt.legend()

plt.figure()
plt.title('Training and validation loss')
plt.plot(epochs, loss, 'blue', label='Training loss')
plt.plot(epochs, val_loss, 'green', label='Validation loss')
plt.legend()

plt.show()